# wrk тестирование 3 реплик

Тестируем 3 реплики, в скриптах установил параметры from и ack равные 3, чтобы учитывать и ожидание ответов от всех реплик.
И PUT и GET запросами выдерживается уже только около 30000 rps:
```
./put.sh 60 30000

Running 1m test @ http://localhost:12345
  6 threads and 64 connections
  Thread calibration: mean lat.: 11.979ms, rate sampling interval: 24ms
  Thread calibration: mean lat.: 10.738ms, rate sampling interval: 19ms
  Thread calibration: mean lat.: 13.134ms, rate sampling interval: 20ms
  Thread calibration: mean lat.: 8.341ms, rate sampling interval: 16ms
  Thread calibration: mean lat.: 7.640ms, rate sampling interval: 21ms
  Thread calibration: mean lat.: 11.218ms, rate sampling interval: 21ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    16.46ms   65.42ms 782.85ms   95.27%
    Req/Sec     5.13k   620.14     8.13k    79.02%
  1799083 requests in 1.00m, 114.95MB read
Requests/sec:  29985.49
Transfer/sec:      1.92MB
```
И GET:
```
./get.sh 60 30000

Running 1m test @ http://localhost:12345
  6 threads and 64 connections
  Thread calibration: mean lat.: 671.389ms, rate sampling interval: 3227ms
  Thread calibration: mean lat.: 808.521ms, rate sampling interval: 4067ms
  Thread calibration: mean lat.: 634.616ms, rate sampling interval: 2717ms
  Thread calibration: mean lat.: 619.641ms, rate sampling interval: 2951ms
  Thread calibration: mean lat.: 485.781ms, rate sampling interval: 2574ms
  Thread calibration: mean lat.: 432.413ms, rate sampling interval: 2347ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.41ms    1.90ms 124.80ms   98.23%
    Req/Sec     5.00k     4.40     5.02k    78.22%
  1799274 requests in 1.00m, 124.78MB read
  Non-2xx or 3xx responses: 661920
Requests/sec:  29988.13
Transfer/sec:      2.08MB
```

Удивительно, что оба видов запросов примерно одинаково отработали - видно теперь сетевые издержки покрыли разницу между ними.
Очень много времени занимает теперь ожидание ответов и агрегация результатов вместе. Так же теперь мы посылали запросы
сразу нескольким репликам, а не одному шарду.

# профилирование 3 реплик

Сразу выделяется область 20% с компилятором. Код мастера, где производится
подсчёт количества пришедших запросов, агрегация разельтата и прочая работа - менее прямолинейный, чем на репликах - видимо,
поэтому такую заметную область компилятор занял. Так с PUT профилем. в GET 5%.
А так большую долю снова заняло http взаимодействие с репликами. Там виден 0,5% __read и 6.7% __writev. Понятно, что у http
заметные накладные расходы. Вероятно, отказавшись от ненужных хедеров, можно было бы уменьшить занимаемое им время.
GET и PUT профили особо не отличаются.

На данном этапе мы пишем *синхронный* сервер, поэтому ответов от реплик приходится ждать какому-то треду. Поскольку ждать
в тредах тред пула мы не можем (так как они могут все забиться), то приходиться ждать и агрегировать ответы в селектор тредах.
Поэтому на профиле SelectorThread и видна бизнес логика.

Аллокации большая часть тоже за httpClient-ом. В 27.5% one.nio.SelectorThread.run 20.5% моего http сервера и сервиса,
а остальные аллокации системные для one nio. А в GET 26.4% заняла база.

64% локов на http клиенте. Но 35% за levelDB. А в GET запросах больше половины за блокировками в базе.

# Выводы

Видно, что снова очень всё упирается в отправку и обработку запросов через http client.
Теперь запрос может проксироваться не только на одну другую ноду, а на любое количетсво. Но в данном кластере это не сильно влияло,
потому что запросы отправляются более менее параллельно и могут обрабатываться тоже параллельно. А агрегация запросов на
малом кластере съедает часть ресурсов, остальное упирается в сетевое взаимодействие.

Переделал хэширование рандеву на консистентное из-за очень неприятного кода, полученного при попытке переделать
алгоритм на возвращение не одной ноды, а нескольких. В рандеву приходилось аллоцировать массив размером с количество нод
и сортировать его. На больших кластерах в это можно теоретически упереться. Думал, сделать один массив, и его переиспользовать,
но тогда нельзя будет обращаться из разных процессов к NodeMapper. Пришла в голову идея предаллоцирорвать каждому треду
свой массив... Пока оставил аллокацию массива каждый раз.
В итоге сделал консистентное.

Изначально написал в 4 домашке сразу реализацию 5 (а именно асинхронное взаимодействие через CompletableFuture
и ожидание только `ack` запросов, а не всех). Но пришлось переделывать и удалять это, а добавлять синхронное взаимодействие.
Но поскольку я сделал реализацию 5 домашки ещё за неделю до её бонусного дедлайна, а именно до дедлайна 4,
то хотелось бы возможности бонусных баллов за это, учитывая то, сколько проблем было распилить асинхронную реализацию
на синхронную. Двойная работа получилась, если не больше.
